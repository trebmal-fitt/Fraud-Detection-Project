{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f485cae",
   "metadata": {},
   "source": [
    "# Data Transform\n",
    "\n",
    "In this notebook, we will ask you a series of questions to evaluate your findings from your EDA. Based on your response & justification, we will ask you to also apply a subsequent data transformation. \n",
    "\n",
    "If you state that you will not apply any data transformations for this step, you must **justify** as to why your dataset/machine-learning does not require the mentioned data preprocessing step.\n",
    "\n",
    "The bonus step is completely optional, but if you provide a sufficient feature engineering step in this project we will add `1000` points to your Kahoot leaderboard score.\n",
    "\n",
    "You will write out this transformed dataframe as a `.csv` file to your `data/` folder.\n",
    "\n",
    "**Note**: Again, note that this dataset is quite large. If you find that some data operations take too long to complete on your machine, simply use the `sample()` method to transform a subset of your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90a38922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "transactions = pd.read_csv(\"../data/bank_transactions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dff7129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      "type              0\n",
      "amount            0\n",
      "nameOrig          0\n",
      "oldbalanceOrg     0\n",
      "newbalanceOrig    0\n",
      "nameDest          0\n",
      "oldbalanceDest    0\n",
      "newbalanceDest    0\n",
      "isFraud           0\n",
      "isFlaggedFraud    0\n",
      "dtype: int64\n",
      "\n",
      "Data types:\n",
      "type               object\n",
      "amount            float64\n",
      "nameOrig           object\n",
      "oldbalanceOrg     float64\n",
      "newbalanceOrig    float64\n",
      "nameDest           object\n",
      "oldbalanceDest    float64\n",
      "newbalanceDest    float64\n",
      "isFraud             int64\n",
      "isFlaggedFraud      int64\n",
      "dtype: object\n",
      "\n",
      "Column names:\n",
      "Index(['type', 'amount', 'nameOrig', 'oldbalanceOrg', 'newbalanceOrig',\n",
      "       'nameDest', 'oldbalanceDest', 'newbalanceDest', 'isFraud',\n",
      "       'isFlaggedFraud'],\n",
      "      dtype='object')\n",
      "\n",
      "Columns after dropping non-predictive ones:\n",
      "Index(['type', 'amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest',\n",
      "       'newbalanceDest', 'isFraud'],\n",
      "      dtype='object')\n",
      "\n",
      "Shape after drop: (1000000, 7)\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(transactions.isnull().sum())\n",
    "\n",
    "# Check data types and head\n",
    "print(\"\\nData types:\")\n",
    "print(transactions.dtypes)\n",
    "\n",
    "# Check column names\n",
    "print(\"\\nColumn names:\")\n",
    "print(transactions.columns)\n",
    "\n",
    "# Drop non-predictive columns (like account names and naive flag)\n",
    "non_predictive_cols = ['nameOrig', 'nameDest', 'isFlaggedFraud']\n",
    "\n",
    "# Drop the columns from the dataset\n",
    "transactions_cleaned = transactions.drop(columns=non_predictive_cols)\n",
    "\n",
    "# Confirm drop\n",
    "print(\"\\nColumns after dropping non-predictive ones:\")\n",
    "print(transactions_cleaned.columns)\n",
    "\n",
    "# Confirm data shape\n",
    "print(\"\\nShape after drop:\", transactions_cleaned.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1360ca62",
   "metadata": {},
   "source": [
    "## Q1\n",
    "\n",
    "Does your model contain any missing values or \"non-predictive\" columns? If so, which adjustments should you take to ensure that your model has good predictive capabilities? Apply your data transformations (if any) in the code-block below.\n",
    "\n",
    " There are **no missing values** in any of the columns, however there are **non-predictive columns** that should be removed to improve model generalization:\n",
    "'nameOrig' and 'nameDest' are account identifiers, which do not contain meaningful patterns generalizable to new data. Including them could lead to overfitting.\n",
    "'isFlaggedFraud' is a naive flag based on a fixed threshold and is not helpful as a predictor since it's already derived from the 'amount' column and performs poorly in flagging actual fraud.\n",
    "\n",
    "To improve predictive capabilities, we can drop these columns. The cleaned dataset now only includes relevant numerical features and the target variable `isFraud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfe64ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "301be5ef",
   "metadata": {},
   "source": [
    "## Q2\n",
    "\n",
    "Do certain transaction types consistently differ in amount or fraud likelihood? If so, how might you transform the type column to make this pattern usable by a machine learning model? Apply your data transformations (if any) in the code-block below.\n",
    "\n",
    "Answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa820db6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b952403f",
   "metadata": {},
   "source": [
    "## Q3\n",
    "\n",
    "After exploring your data, you may have noticed that fraudulent transactions are rare compared to non-fraudulent ones. What challenges might this pose when training a machine learning model? What strategies could you use to ensure your model learns meaningful patterns from the minority class? Apply your data transformations (if any) in the code-block below.\n",
    "\n",
    "Answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22f1422",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17737e9e",
   "metadata": {},
   "source": [
    "## Bonus (optional)\n",
    "\n",
    "Are there interaction effects between variables (e.g., fraud and high amount and transaction type) that aren't captured directly in the dataset? Would it be helpful to manually engineer any new features that reflect these interactions? Apply your data transformations (if any) in the code-block below.\n",
    "\n",
    "Answer Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f48b7af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81cbfb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out newly transformed dataset to your folder\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
